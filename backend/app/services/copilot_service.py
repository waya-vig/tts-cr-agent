"""Copilot chat service with RAG knowledge retrieval and DB persistence."""

import json
import logging
import uuid
from collections.abc import AsyncGenerator
from datetime import datetime, timezone

from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.config import settings
from app.models.conversation import ChatMessage, Conversation
from app.models.global_knowledge import GlobalKnowledge
from app.models.knowledge_base import KnowledgeBase
from app.schemas.copilot import CopilotMessage, CopilotResponse
from app.services.ai_client import get_async_client, get_model_id

logger = logging.getLogger(__name__)


async def _retrieve_knowledge(
    user_id: uuid.UUID,
    query: str,
    db: AsyncSession,
    global_limit: int = 3,
    user_limit: int = 3,
) -> tuple[list[dict], list[dict]]:
    """Two-layer semantic knowledge retrieval via Pinecone.

    Searches both:
      1. Global namespace (admin-managed common knowledge)
      2. User namespace (per-user knowledge)

    Returns (global_knowledge, user_knowledge).
    Falls back to DB-based retrieval if vector search is unavailable.
    """
    if settings.pinecone_api_key:
        try:
            from app.services.embedding_service import generate_embedding
            from app.services.pinecone_service import query_both_namespaces

            query_embedding = await generate_embedding(query, input_type="search_query")
            global_matches, user_matches = await query_both_namespaces(
                embedding=query_embedding,
                user_id=user_id,
                global_top_k=global_limit,
                user_top_k=user_limit,
            )

            global_knowledge = await _matches_to_knowledge(
                global_matches, db, is_global=True
            )
            user_knowledge = await _matches_to_knowledge(
                user_matches, db, is_global=False
            )
            return global_knowledge, user_knowledge

        except Exception as e:
            logger.warning(f"Vector search failed, falling back to DB: {e}")

    # Fallback: DB-based retrieval (user knowledge only, no global table yet may be empty)
    global_knowledge = await _fallback_global_knowledge(db, global_limit)
    user_knowledge = await _fallback_user_knowledge(user_id, db, user_limit)
    return global_knowledge, user_knowledge


async def _matches_to_knowledge(
    matches: list[dict],
    db: AsyncSession,
    is_global: bool,
) -> list[dict]:
    """Convert Pinecone matches to knowledge dicts by fetching from DB."""
    if not matches:
        return []

    knowledge_ids = [uuid.UUID(m["id"]) for m in matches]
    model = GlobalKnowledge if is_global else KnowledgeBase

    result = await db.execute(
        select(model).where(model.id.in_(knowledge_ids))
    )
    entries = {str(e.id): e for e in result.scalars().all()}

    knowledge = []
    for match in matches:
        entry = entries.get(match["id"])
        if entry:
            item = {
                "title": entry.title,
                "content": entry.content,
                "source": "global" if is_global else "user",
            }
            if not is_global and hasattr(entry, "category") and entry.category:
                item["category"] = entry.category.value
            knowledge.append(item)
    return knowledge


async def _fallback_global_knowledge(db: AsyncSession, limit: int) -> list[dict]:
    """DB fallback for global knowledge."""
    result = await db.execute(
        select(GlobalKnowledge)
        .order_by(GlobalKnowledge.created_at.desc())
        .limit(limit)
    )
    return [
        {"title": e.title, "content": e.content, "source": "global"}
        for e in result.scalars().all()
    ]


async def _fallback_user_knowledge(
    user_id: uuid.UUID, db: AsyncSession, limit: int
) -> list[dict]:
    """DB fallback for user knowledge."""
    result = await db.execute(
        select(KnowledgeBase)
        .where(KnowledgeBase.user_id == user_id)
        .order_by(KnowledgeBase.performance_score.desc().nullslast())
        .limit(limit)
    )
    return [
        {
            "title": e.title,
            "content": e.content,
            "source": "user",
            "category": e.category.value if e.category else None,
        }
        for e in result.scalars().all()
    ]


def _build_system_prompt(
    global_knowledge: list[dict],
    user_knowledge: list[dict],
) -> str:
    base = """ã‚ãªãŸã¯TikTok Shopé‹å–¶ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®ãƒŠãƒ¬ãƒƒã‚¸ã‚’å‚è€ƒã«ã—ãªãŒã‚‰å…·ä½“çš„ã§å®Ÿç”¨çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

## TikTokã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ï¼ˆCRï¼‰ã®å¤§å‰æ
TikTokã®CRã¯ãƒ–ãƒ©ãƒ³ãƒ‰CMã‚„MVé¢¨ã®æ˜ åƒã§ã¯ãªã„ã€‚ä»¥ä¸‹ã®ç‰¹å¾´ã‚’å¸¸ã«å¿µé ­ã«ç½®ãã“ã¨:
- UGCï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ç”Ÿæˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼‰æ„ŸãŒæœ€é‡è¦ã€‚ã‚¹ãƒžãƒ›æ’®å½±ã€è‡ªç„¶å…‰ã€ç”Ÿæ´»æ„Ÿã®ã‚ã‚‹ãƒªã‚¢ãƒ«ãªæ˜ åƒ
- ã€Œåºƒå‘Šã£ã½ã•ã€ã¯æœ€å¤§ã®æ•µã€‚è¦–è´è€…ã¯åºƒå‘Šã¨æ„Ÿã˜ãŸçž¬é–“ã«ã‚¹ãƒ¯ã‚¤ãƒ—ã™ã‚‹
- æ¼”è€…ã¯ã€Œå‹é”ãŒãŠã™ã™ã‚ã—ã¦ãã‚Œã¦ã‚‹ã€æ„Ÿè¦šã€‚ãƒ—ãƒ­ã£ã½ã„æ¼”å‡ºã‚ˆã‚Šç´ äººæ„Ÿãƒ»ãƒªã‚¢ãƒ«æ„Ÿ
- å•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ç³»ã¯ã€Œå®Ÿéš›ã«ä½¿ã£ã¦ã¿ãŸã€ã€Œè²·ã£ã¦ã¿ãŸã€ã®ãƒˆãƒ¼ãƒ³
- æ’®å½±ã¯è‡ªå®…ãƒ»æ—¥å¸¸ã‚·ãƒ¼ãƒ³ãŒåŸºæœ¬ã€‚ã‚¹ã‚¿ã‚¸ã‚ªæ’®å½±ã‚„ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°æ©Ÿæã¯ä¸è¦
- ãƒ†ãƒ­ãƒƒãƒ—ã¯æ‰‹ä½œã‚Šæ„Ÿã®ã‚ã‚‹ãƒ•ã‚©ãƒ³ãƒˆã€ãƒãƒƒãƒ—ãªè‰²ä½¿ã„
- BGMã¯TikTokã§ãƒˆãƒ¬ãƒ³ãƒ‰ã®æ¥½æ›²ã‚’ä½¿ã†
- å°ºã¯15ã€œ60ç§’ãŒä¸»æµã€‚é•·ãã¦ã‚‚90ç§’ä»¥å†…
- å†’é ­1ã€œ2ç§’ã®ãƒ•ãƒƒã‚¯ãŒå…¨ã¦ã€‚ã“ã“ã§æ­¢ã¾ã‚‰ãªã‘ã‚Œã°è¦‹ã¦ã‚‚ã‚‰ãˆãªã„

å›žç­”ã®ãƒ«ãƒ¼ãƒ«:
- æ—¥æœ¬èªžã§å›žç­”ã™ã‚‹
- äº‹å®Ÿãƒ™ãƒ¼ã‚¹ã§ç°¡æ½”ã«å›žç­”ã™ã‚‹
- ã€Œã”ç›¸è«‡ã‚’å—ã‘ã¦ã„ã¾ã™ã€ã®ã‚ˆã†ãªæž¶ç©ºã®çŠ¶æ³è¨­å®šã¯ã—ãªã„
- CRï¼ˆã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ï¼‰ã«ã¤ã„ã¦èžã‹ã‚ŒãŸå ´åˆã€ä¸Šè¨˜ã®TikTokã‚‰ã—ã•ã‚’å‰æã«ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã™ã‚‹ã€‚ãƒ–ãƒ©ãƒ³ãƒ‰CMçš„ãªææ¡ˆã¯ã—ãªã„
- ãƒŠãƒ¬ãƒƒã‚¸ã«ãªã„å†…å®¹ã‚’èžã‹ã‚ŒãŸå ´åˆã¯ã€çŸ¥ã£ã¦ã„ã‚‹ç¯„å›²ã§å›žç­”ã—ã€ã‚ã‹ã‚‰ãªã„ã“ã¨ã¯æ­£ç›´ã«ä¼ãˆã‚‹
- ç®‡æ¡æ›¸ãã‚„è¦‹å‡ºã—ã‚’æ´»ç”¨ã—ã¦èª­ã¿ã‚„ã™ãæ•´ç†ã™ã‚‹
- è¦‹å‡ºã—ã‚„ç®‡æ¡æ›¸ãã«çµµæ–‡å­—ï¼ˆðŸ”¥âœ…ðŸ’¡ðŸ“ŒðŸŽ¯ãªã©ï¼‰ã‚’ç©æ¥µçš„ã«ä½¿ã„ã€è¦–è¦šçš„ã«èª­ã¿ã‚„ã™ãã™ã‚‹
- è¦ªã—ã¿ã‚„ã™ã„ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªãƒˆãƒ¼ãƒ³ã§è©±ã™"""

    if global_knowledge:
        base += "\n\n## å…±é€šãƒŠãƒ¬ãƒƒã‚¸ï¼ˆTikToké‹å–¶ãƒŽã‚¦ãƒã‚¦ï¼‰\n"
        for k in global_knowledge:
            base += f"\n### {k['title']}"
            base += f"\n{k['content']}\n"

    if user_knowledge:
        base += "\n\n## ãƒ¦ãƒ¼ã‚¶ãƒ¼ç‹¬è‡ªãƒŠãƒ¬ãƒƒã‚¸\n"
        for k in user_knowledge:
            base += f"\n### {k['title']}"
            if k.get("category"):
                base += f" ({k['category']})"
            base += f"\n{k['content']}\n"

    return base


async def _get_or_create_conversation(
    user_id: uuid.UUID,
    conversation_id: str | None,
    first_message: str,
    db: AsyncSession,
) -> Conversation:
    """Get existing conversation or create a new one."""
    if conversation_id:
        result = await db.execute(
            select(Conversation).where(
                Conversation.id == uuid.UUID(conversation_id),
                Conversation.user_id == user_id,
            )
        )
        conv = result.scalar_one_or_none()
        if conv:
            conv.updated_at = datetime.now(timezone.utc)
            return conv

    # Create new conversation with title from first message
    title = first_message[:30].strip()
    if len(first_message) > 30:
        title += "..."
    conv = Conversation(user_id=user_id, title=title)
    db.add(conv)
    await db.flush()
    return conv


async def _load_history(conversation_id: uuid.UUID, db: AsyncSession) -> list[dict]:
    """Load message history from DB for a conversation."""
    result = await db.execute(
        select(ChatMessage)
        .where(ChatMessage.conversation_id == conversation_id)
        .order_by(ChatMessage.created_at)
    )
    msgs = result.scalars().all()
    return [{"role": m.role, "content": m.content} for m in msgs]


async def _save_message(
    conversation_id: uuid.UUID,
    role: str,
    content: str,
    sources: list[str] | None,
    db: AsyncSession,
) -> ChatMessage:
    """Save a single message to the DB."""
    msg = ChatMessage(
        conversation_id=conversation_id,
        role=role,
        content=content,
        sources=sources,
    )
    db.add(msg)
    await db.flush()
    return msg


async def chat_sync(
    user_id: uuid.UUID,
    message: str,
    history: list[CopilotMessage],
    db: AsyncSession,
    conversation_id: str | None = None,
) -> CopilotResponse:
    """Non-streaming chat completion with DB persistence."""
    global_knowledge, user_knowledge = await _retrieve_knowledge(user_id, message, db)

    # Get or create conversation
    conv = await _get_or_create_conversation(user_id, conversation_id, message, db)

    # Load history from DB if continuing a conversation
    if conversation_id:
        messages = await _load_history(conv.id, db)
    else:
        messages = [{"role": m.role, "content": m.content} for m in history]

    messages.append({"role": "user", "content": message})

    # Save user message
    await _save_message(conv.id, "user", message, None, db)

    client = get_async_client()
    response = await client.messages.create(
        model=get_model_id(),
        max_tokens=2048,
        system=_build_system_prompt(global_knowledge, user_knowledge),
        messages=messages,
    )

    reply = response.content[0].text
    all_knowledge = global_knowledge + user_knowledge
    sources = [k["title"] for k in all_knowledge]

    # Save assistant message
    await _save_message(conv.id, "assistant", reply, sources, db)

    return CopilotResponse(
        reply=reply,
        sources=sources,
        conversation_id=str(conv.id),
    )


async def chat_stream(
    user_id: uuid.UUID,
    message: str,
    history: list[CopilotMessage],
    db: AsyncSession,
    conversation_id: str | None = None,
) -> AsyncGenerator[str, None]:
    """Streaming chat using Server-Sent Events with DB persistence."""
    global_knowledge, user_knowledge = await _retrieve_knowledge(user_id, message, db)

    # Get or create conversation
    conv = await _get_or_create_conversation(user_id, conversation_id, message, db)

    # Load history from DB if continuing a conversation
    if conversation_id:
        messages = await _load_history(conv.id, db)
    else:
        messages = [{"role": m.role, "content": m.content} for m in history]

    messages.append({"role": "user", "content": message})

    # Save user message
    await _save_message(conv.id, "user", message, None, db)

    client = get_async_client()

    # Send conversation_id and sources first
    all_knowledge = global_knowledge + user_knowledge
    sources = [k["title"] for k in all_knowledge]
    yield f"data: {json.dumps({'type': 'conversation_id', 'conversation_id': str(conv.id)})}\n\n"
    yield f"data: {json.dumps({'type': 'sources', 'sources': sources})}\n\n"

    assistant_content = ""
    async with client.messages.stream(
        model=get_model_id(),
        max_tokens=2048,
        system=_build_system_prompt(global_knowledge, user_knowledge),
        messages=messages,
    ) as stream:
        async for text in stream.text_stream:
            assistant_content += text
            yield f"data: {json.dumps({'type': 'text', 'content': text})}\n\n"

    # Save assistant message after streaming completes
    await _save_message(conv.id, "assistant", assistant_content, sources, db)

    yield f"data: {json.dumps({'type': 'done'})}\n\n"
